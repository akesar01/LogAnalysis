{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Elastic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a client instance of the library\n",
    "es = Elasticsearch(host='localhost', port=9200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to fetch the Ids and their Respective Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_elastic():\n",
    "    # query: The elasticsearch query.\n",
    "    query = {\n",
    "    \n",
    "    }\n",
    "    # Scan function to get all the data. \n",
    "    response = scan(client=es,             \n",
    "               query=query,                                     \n",
    "               scroll='2m',\n",
    "               index='*.27',\n",
    "               raise_on_error=True,\n",
    "               preserve_order=False,\n",
    "               clear_scroll=True)\n",
    "\n",
    "    # Keep response in a list.\n",
    "    result = list(response)\n",
    "    print(\"! \",result)\n",
    "    ids =[]\n",
    "    for i in range(len(result)):\n",
    "        ids.append(result[i][\"_id\"])\n",
    "    temp = []\n",
    "    \n",
    "    # We need only '_source', which has all the fields required.\n",
    "    for hit in result:\n",
    "        \n",
    "        temp.append(hit['_source'])\n",
    "    # Create a dataframe.\n",
    "    df = pd.DataFrame(temp)\n",
    "\n",
    "    return (ids,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = get_data_from_elastic()\n",
    "ids = lol[0]\n",
    "df = lol[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First 1000 data from the DataFrame\n",
    "df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meassage Example\n",
    "df['message'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the message list from the dataFrame for Text Cleaning, Tokenization,etc.\n",
    "msg_list = list(df['message'])\n",
    "for i in range(len(msg_list)):\n",
    "    print(msg_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in msg_list:\n",
    "    #To replace \\220 with another value\n",
    "    msg.replace('\\220','>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "clean_msg_list=[]\n",
    "for msg in msg_list:\n",
    "    #various regular expressions to clean the message data\n",
    "    #To clear the hyperlinks\n",
    "    msg = re.sub(r'https?:\\/\\/.\\S+', \"\", msg)\n",
    "    #To clean the date and time from the msgs\n",
    "    msg = re.sub(r'(\\S\\S \\S\\S\\S )?\\S\\S\\S( )? (\\d)?\\d \\d\\d:\\d\\d:\\d\\d (\\d\\d\\d\\d)?','',msg)\n",
    "    #To remove various emailIds\n",
    "    msg = re.sub(r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$','',msg)\n",
    "    #To remove ... special characters from the msgs\n",
    "    msg = re.sub('\\...+','',msg)\n",
    "    #To remove � from the msgs\n",
    "    msg = re.sub(r'\\S\\�+','',msg)\n",
    "    #To remove any substring of #%abc42 type\n",
    "    msg = re.sub('\\%\\w+','',msg)\n",
    "    #To remove the word combo from the msgs\n",
    "    msg = re.sub('combo',\"\",msg)\n",
    "    #To remove Blank Lines\n",
    "    msg = re.sub(r'\\n\\s*\\r ','',msg)\n",
    "    #To remove punctuation\n",
    "    msg = re.sub(r'[^\\w\\s]','',msg)\n",
    "    #To remove digits from the messages\n",
    "    msg = re.sub(r'\\d+','',msg)\n",
    "    #To remove any html links\n",
    "    msg= html.unescape(msg)\n",
    "    clean_msg_list.append(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_msg_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word tokenization\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "stop_words = stop_words+['logname', 'uid','eth','euid','PCI','node','Privacy','IRQ','ttyNODEVssh', 'ruser','rhostjwhp5','IPv','tables','Microcode', 'Driver','Hash', 'interface','family','httpd','info', 'mice','check','pass','cache','spamd','bytes','syslog','klogd','BIOS','protocol','NET','md','arrays','filesystem','CPU','user', 'cyrus','dev', 'type', 'ext3', 'uses','others','internal', 'service','already','floppy','syslogd','ip_tables','startup','use','NET', 'Registered', 'Revision','CDROM', 'drive','hda']\n",
    "new_Doc=[]\n",
    "for msg in clean_msg_list:\n",
    "    nltk_tokens = nltk.word_tokenize(msg)\n",
    "    filtered_sentence = []\n",
    "    for w in nltk_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    new_Doc.append(filtered_sentence)\n",
    "print(new_Doc)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More research on domain stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer as wl\n",
    "wordnet = wl()\n",
    "\n",
    "doc =[]\n",
    "for docs in new_Doc:\n",
    "    arr =[]\n",
    "    for word in docs:\n",
    "        arr.append(wordnet.lemmatize(word))\n",
    "    doc.append(arr)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Formation/DeTokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacremoses import MosesDetokenizer\n",
    "\n",
    "parsed_logs=[]\n",
    "for words in doc:\n",
    "    detokens = MosesDetokenizer().detokenize(words, return_str=True)\n",
    "    parsed_logs.append(detokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df['agent'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open(r\"C:/Users/akesar01/Documents/Logs/ParsedLogs.txt\", \"r+\")\n",
    "textfile.write('id'+\",\"+'message'+\",\"+'@version'+\",\"+'@timestamp'+\"\\n\")\n",
    "for i in range(len(parsed_logs)):\n",
    "    textfile.write(ids[i]+\",\"+parsed_logs[i]+\",\"+df['@version'][i]+\",\"+df['@timestamp'][i]+\"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = es.get(index=\"filebeat-7.16.3-2022.03.10\", id=ids[0])\n",
    "print(resp['_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_logs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = es.get(index=\"gamma_2\", id= 'Vy7ajH8BLOYn9AgzScoh')\n",
    "print(resp['_source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Differentiation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose of TF - IDF is to highlight words which are frequent  in a document but not cross document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = parsed_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = [(k, vectorizer.vocabulary_[k]) for k in vectorizer.vocabulary_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(lol,key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = result.toarray()\n",
    "for i in range(len(arr)):\n",
    "    for j in range(len(arr[0])):\n",
    "        print(arr[i][j], end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize encoded vector\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nidf values:')\n",
    "c=zip(vectorizer.get_feature_names(), vectorizer.idf_)\n",
    "for ele1, ele2 in sorted(c,key = lambda x: x[1]):\n",
    "    print(ele1, ':', ele2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "777ac3220d56c49a96724f5060994578b9f53f3d4bdd42da3b08b6165829f144"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
